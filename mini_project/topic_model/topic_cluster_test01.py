# -*- coding: utf-8 -*-
from gensim import corpora, models, similarities
import numpy as np
from sklearn import cluster
from sklearn.preprocessing import StandardScaler
from collections import defaultdict
from scipy import *  
from sys import argv
import jieba, codecs, datetime, time, re, xlrd
import sys 
reload(sys) # Python2.5 åˆå§‹åŒ–åä¼šåˆ é™¤ sys.setdefaultencoding è¿™ä¸ªæ–¹æ³•ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°è½½å…¥ 
sys.setdefaultencoding('utf-8') 

documents = []
hidden_vectors = []

# ä¸åŒä¸šåŠ¡éœ€è¦é…ç½®ä¸åŒçš„è¯å…¸ï¼Œæ•ˆæœæ¯”è¾ƒå¥½
# åœ¨åœæ­¢è¯ä¸­æ·»åŠ çš„å†…å®¹ï¼ŒåŒæ ·éœ€è¦æ·»åŠ åˆ°å¯¹åº”çš„è¯å…¸ä¸­ï¼Œè¿™æ ·æ‰èƒ½åˆ†è¯å‡ºæ¥
# æ›´æ–°è¯è¡¨åï¼Œéœ€è¦é‡æ–°åˆ†è¯

query_answer_rawdata = "knowledge_data/baiyushuang_result.txt" # ç”¨æ¥å¤„ç†é—®ç­”å¯¹çš„åŸå§‹æ•°æ®
query_answer_result = "qa/result.txt" # å¤„ç†åçš„é—®ç­”å¯¹
query_answer_qfile = "qa/quesiton.txt" # é—®é¢˜åˆé›†
query_answer_afile = "qa/answer.txt" # ç­”æ¡ˆåˆé›†

user_dict = "dict/default_dict.txt"
#user_dict = "dict/zhaohang_dict.txt"
#user_dict = "dict/nanfang_dict.txt"
#user_dict = "dict/pinduoduo_dict.txt"

word_freq_file = "stat/word_freq.txt" # è¯é¢‘ç»Ÿè®¡

split_query = "query/query_zhaohang_split" # æ–¹å¤ªåˆ†è¯åçš„æ–‡ä»¶

raw_query = "query/zhaohang.txt" # æ–¹å¤ª
# raw_query = "query/query_5" # æ‹›è¡Œ
# raw_query = "query/miya.txt" # å¯†èŠ½ FAQ
# raw_query = "query/clear2" # æœ‰ç”¨åˆ†æœŸ
# raw_query = "query/query_1" # å—æ–¹

knowledge_data = "knowledge_data/baiyushuang.xls"
knowledge_result = "knowledge_data/baiyushuang_result.txt"


hidden_vector = "query/zhaohang.hidden" # æ–¹å¤ªçš„éšè—å‘é‡zhaohangˆ fangtai.txt
# hidden_vector = "query/xiaomi.hidden" # è‡ªå·±è®­ç»ƒçš„å°ç±³å‘é‡ï¼Œé…åˆ xiaomi.txtï¼Œå…± 15w æ•°æ®
# hidden_vector = "query/nanfang.other_query.hidden_vec" # æ¨è€å¸ˆç»™çš„å—æ–¹çš„å‘é‡ï¼Œé…åˆ query_1
# raw_query = "query/query_9" # æ‹¼å¤šå¤š

dict_path = "query/zhaohang.dict"
#dict_path = "query/zhaohang.dict"
#dict_path = "query/miya.dict"
#dict_path = "query/fenqi.dict"
#dict_path = "query/nanfang.dict"
#dict_path = "query/pinduoduo.dict"

num_topic = 500

topic_dict_path = "query/topic%d.dict"
tokenized_query = "query/tokenized_query"
topic_query = "query/topic_query" # æ¯ä¸ª topic çš„ queryï¼Œç”¨äºäºŒæ¬¡ LDA èšç±»
topic_result = "topic_result"
lda_model = "model/lda_model"
lda_model_topic = "model/lda_model_topic"
lsi_model = "model/lsi_model"
stop_dict = "dict/stop_words.txt"

lda_result_dir = "lda_result/"
lsi_result_dir = "lsi_result/"
kmeans_result_dir = "kmeans_result/"
ap_result_dir = "ap_result/"

num_sub_topic = 0
num_topn = 40 # get topic terms çš„è¯ä¸ªæ•°
final_a = 200 # topic è¯„åˆ†ä¸­ doc çš„ç³»æ•°
final_b = 100 # topic è¯„åˆ†ä¸­ word çš„ç³»æ•°
min_word_count = 15 # åˆ†æœŸçš„æ•°æ®äººåè¾ƒå¤šï¼Œè¿‡æ»¤æ‰
max_sentence_length = 400 # è¿™ä¸ªå€¼å¦‚æœæ•°å€¼å¤§
show_count = 2000 # å¤„ç†å¤šå°‘æ¡è®°å½•æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦

# ä¿è¯æ¯æ¬¡ç»“æœä¸€è‡´
FIXED_SEED = 44 
np.random.seed(FIXED_SEED)

jieba.set_dictionary('/data/wdxtub/machine-learning-projects/topic_model/dict.txt.small')
jieba.load_userdict(user_dict)
time.sleep(1)

# KL æ•£åº¦
# éœ€è¦ä¼ å…¥ä¿© numpy array
def asymmetricKL(P,Q):  
    return sum(P * log(P / Q)) #calculate the kl divergence between P and Q  

# KL æ•£åº¦
def symmetricalKL(P,Q):  
    return (asymmetricKL(P,Q)+asymmetricKL(Q,P))/2.00  

def tokenize_query():
    print "å¼€å§‹è¯»å–åŸå§‹ç”¨æˆ·é—®å¥æ—¥å¿—ï¼Œä¹‹ååˆ†è¯å¹¶ä¿å­˜åˆ°æ–‡ä»¶ä¸­"
    count = 0
    jieba.load_userdict(user_dict)
    with open(raw_query) as fr:
        for line in fr:
            if (len(line) < min_word_count):
                # å¤„ç†ç©ºè¡Œå’Œé•¿åº¦å°äº 6 çš„
                continue
            if (len(line) > max_sentence_length):
                # å¤ªé•¿çš„å»æ‰
                continue
            # å»æ‰ç©ºæ ¼
            arr = line.split(" ")
            seg_list = jieba.cut("".join(arr))  # é»˜è®¤æ˜¯ç²¾ç¡®æ¨¡å¼
            documents.append(" ".join(seg_list))
            count = count + 1
            if (count % 10000 == 0):
                print "å·²å¤„ç† %d è¡Œ" % count
    print "æ—¥å¿—å…± %d æ¡æœ‰æ•ˆè®°å½•" % count
    with codecs.open(tokenized_query, "w+", "utf-8") as f:
        for doc in documents:
            f.write(doc)
    print "å†™å…¥åˆ°æ–‡ä»¶ %s å®Œæˆ" % tokenized_query

def load_raw_query():
    print "è½½å…¥åŸå§‹æ•°æ®ï¼ˆä¸åˆ†è¯ä¸è¿‡æ»¤ï¼‰"
    count = 0
    with open(raw_query) as fr:
        for line in fr:
            # å¤„ç†ä¸‹æ¯ä¸€è¡Œï¼Œå¦‚æœæ˜¯åˆ†è¯åçš„ï¼Œåˆèµ·æ¥ï¼Œå¦‚æœæœ‰æ¢è¡Œï¼Œå»æ‰
            arr = line.split(' ')
            newline = "".join(arr)
            newline = newline.replace("\n","")
            newline = newline.replace("\r","")
            documents.append(newline)
            count = count + 1
    print "å…± %d æ¡æ—¥å¿—è®°å½•" % count

def load_hidden_vector():
    print "è½½å…¥å¯¹åº” %s çš„éšè—å‘é‡" % raw_query
    count = 0
    with open(hidden_vector) as fr:
        for line in fr:  
            hidden_vectors.append(line)
            count = count + 1
    print "å…± %d ä¸ªéšè—å‘é‡" % count

def load_tokenized_query():
    print "è½½å…¥å¤„ç†åçš„åˆ†è¯æ•°æ®"
    with open(tokenized_query) as fr:
        for line in fr:
            documents.append(line)
    print "æ—¥å¿—å…± %d æ¡æœ‰æ•ˆè®°å½•" % len(documents)

def load_topic_query(topicid):
    print "è½½å…¥ topic %d çš„åˆ†è¯æ•°æ®" % topicid
    count = 0
    topic_docs = []
    with open("%s%d" % (topic_query, topicid)) as f:
        for line in f:
            topic_docs.append(line)
            count = count + 1
    print "topic %d å…±æœ‰ %d æ¡æœ‰æ•ˆè®°å½•" % (topicid, len(topic_docs))
    return topic_docs

def get_stoplist():
    stoplist = set()
    # è½½å…¥åœæ­¢è¯
    with codecs.open(stop_dict, "r+", "utf-8") as f:
        for word in f:
            stoplist.add(word[:-1]) # å»æ‰æœ€åçš„å›è½¦ç¬¦å·
    print "åœæ­¢è¯å…± %d ä¸ª" % len(stoplist)
    return stoplist

def get_texts():
    stoplist = get_stoplist()
    # remove words and tokenize
    texts = [[word for word in document.lower().split() if word not in stoplist] 
            for document in documents]
    frequency = defaultdict(int)
    for text in texts:
        for token in text:
            frequency[token] += 1

    # for i in range(10):
    #     print " ".join(texts[i])
    
    # remove words that appear only once
    texts = [[token for token in text if frequency[token] > 1]
            for text in texts]
    
    # å»æ‰é•¿åº¦å¤§äº 6 çš„è¯
    texts = [[token for token in text if len(token) <= 6]
           for text in texts]

    # åˆ æ‰ç©ºè¡Œ
    tmp = []
    for text in texts:
        # è‡³å°‘éœ€è¦ä¿©è¯
        if (len(text) > 1):
            tmp.append(text)
    
    texts = tmp

    print "å…¨é‡è¯­æ–™å·²è½½å…¥ï¼Œå…± %d æ¡" % len(texts)
    return texts

def get_topic_texts(topicid):
    stoplist = get_stoplist()
    topic_docs = load_topic_query(topicid)
    texts = [[unicode(word, "utf-8") for word in doc.lower().split() if unicode(word, "utf-8") not in stoplist] 
            for doc in topic_docs]
    frequency = defaultdict(int)
    for text in texts:
        for token in text:
            frequency[token] += 1
    
    # remove words that appear only once
    texts = [[token for token in text if frequency[token] > 1]
            for text in texts]
    
    # å»æ‰é•¿åº¦å¤§äº 6 çš„è¯
    texts = [[token for token in text if len(token) <= 6]
           for text in texts]

    # åˆ æ‰ç©ºè¡Œ
    tmp = []
    for text in texts:
        # è‡³å°‘éœ€è¦ä¿©è¯
        if (len(text) > 1):
            tmp.append(text)
    texts = tmp

    print "topic %d çš„è¯­æ–™å·²è½½å…¥ï¼Œå…± %d æ¡" % (topicid, len(texts))
    return texts


def generate_dict():
    texts = get_texts()
    # store the dictionary for future referece
    dictionary = corpora.Dictionary(texts)
    dictionary.save(dict_path) 
    print "è¯å…¸å·²ä¿å­˜åˆ° %s æ–‡ä»¶ä¸­" % dict_path

def generate_topic_dict(topicid):
    texts = get_topic_texts(topicid)
    dictionary = corpora.Dictionary(texts)
    path = topic_dict_path % topicid
    dictionary.save(path)
    print "topic %d è¯å…¸å·²ä¿å­˜åˆ° %s æ–‡ä»¶ä¸­" % (topicid, path)

def generate_lsi_topic():
     # è½½å…¥è¯å…¸
    dictionary = corpora.Dictionary.load(dict_path)
    print "è½½å…¥è¯å…¸å®Œæˆ"

    # è½½å…¥è¯­æ–™
    texts = get_texts()

    begin = datetime.datetime.now()
    corpus = [dictionary.doc2bow(text) for text in texts]

    print "å¼€å§‹è®­ç»ƒ LSI æ¨¡å‹ï¼Œå…± %d ä¸ª topic" % num_topic
    LSI = models.LsiModel(corpus, num_topics=num_topic, id2word=dictionary, chunksize=2000)

    end = datetime.datetime.now()
    print "å¤„ç† LSI ç”¨æ—¶", end - begin

    # ä¿å­˜ LDA æ¨¡å‹
    LSI.save(lsi_model)
    print "æ¨¡å‹å·²ä¿å­˜åˆ° %s ä¸­" % lsi_model

def generate_lda_sub_topic(topicid):
    # è½½å…¥è¯å…¸
    dictionary = corpora.Dictionary.load(topic_dict_path % topicid)
    print "è½½å…¥ topic %d è¯å…¸å®Œæˆ" % topicid

    # è½½å…¥è¯­æ–™
    texts = get_topic_texts(topicid)

    begin = datetime.datetime.now()
    corpus = [dictionary.doc2bow(text) for text in texts]
    # store to disk, for later use
    # corpora.MmCorpus.serialize('./nanfang.mm', corpus)  
    # å•æ ¸
    # LDA = models.LdaModel(corpus, id2word=dictionary, num_topics=200, update_every=1, minimum_probability=0.1, passes=5)
    # å¤šæ ¸
    # models.ldamulticore.LdaMulticore(corpus, num_topics=200, id2word=dictionary, workers=None, chunksize=2000, passes=1, batch=False, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001)
    print "å¼€å§‹è®­ç»ƒ topic %d çš„å­ topicï¼Œå…± %d ä¸ª" % (topicid,  num_sub_topic)
    LDA = models.LdaMulticore(corpus, num_topics=num_sub_topic, id2word=dictionary, workers=4, chunksize=2000, passes=1)
    end = datetime.datetime.now()
    print "è®­ç»ƒç”¨æ—¶", end - begin

    # ä¿å­˜ LDA æ¨¡å‹
    path = "%s%d" % (lda_model_topic, topicid)
    LDA.save(path)
    print "topic %d æ¨¡å‹å·²ä¿å­˜åˆ° %s ä¸­\n" % (topicid, path)

def generate_lda_topic():
    # è½½å…¥è¯å…¸
    dictionary = corpora.Dictionary.load(dict_path)
    print "è½½å…¥è¯å…¸å®Œæˆ"

    # è½½å…¥è¯­æ–™
    texts = get_texts()

    begin = datetime.datetime.now()
    corpus = [dictionary.doc2bow(text) for text in texts]
    # store to disk, for later use
    # corpora.MmCorpus.serialize('./nanfang.mm', corpus)  
    # å•æ ¸
    # LDA = models.LdaModel(corpus, id2word=dictionary, num_topics=200, update_every=1, minimum_probability=0.1, passes=5)
    # å¤šæ ¸
    # models.ldamulticore.LdaMulticore(corpus, num_topics=200, id2word=dictionary, workers=None, chunksize=2000, passes=1, batch=False, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001)
    print "å¼€å§‹è®­ç»ƒç¬¬ä¸€å±‚ LDA æ¨¡å‹ï¼Œå…± %d ä¸ª topic" % num_topic
    LDA = models.LdaMulticore(corpus, num_topics=num_topic, id2word=dictionary, workers=4, chunksize=2000, passes=5)
    level1 = datetime.datetime.now()
    print "ç¬¬ä¸€å±‚ LDA æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶", level1 - begin
    
    # åˆ†ç¦»æ¯ä¸ª topic çš„æ•°æ®
    # topic_text = [[] for i in range(num_topic)]
    # print "å¼€å§‹åˆ†ç¦»å„ä¸ª topic çš„æ•°æ®"
    # for i in range(len(texts)):
    #     if (i % 10000 == 0):
    #         print "æ­£åœ¨å¤„ç†ç¬¬ %d è¡Œ" % i 
    #     # è·å–æ¯ä¸ªæ–‡æœ¬çš„ topics
    #     topics = LDA.get_document_topics(corpus[i])
    #     # è¿™é‡Œé€‰æ‹© Top1
    #     if (len(topics) < 1):
    #         continue
    #     # print len(topics), topics[0]
    #     topic_text[topics[0][0]].append(" ".join(texts[i]))    
    
    # å†™å…¥æ¯ä¸ª topic çš„æ•°æ®
    # for i in range(num_topic):
    #     print "å†™å…¥ topic %d çš„ç”¨æˆ·é—®å¥ï¼ˆå·²åˆ†è¯ï¼‰" % i
    #     with codecs.open("%s%d" %(topic_query, i), "w+", "utf-8") as f:
    #         for line in topic_text[i]:
    #             if (len(line) > 1):
    #                 f.write(line+"\n")

    # å†™å…¥å­ topic
    # for i in range(num_topic):
    #     generate_topic_dict(i) # ç”Ÿæˆè¯å…¸
    #     generate_sub_topic(i) # ç”Ÿæˆ å­ topic

    end = datetime.datetime.now()
    print "å¤„ç† LDA ç”¨æ—¶", end - begin

    # ä¿å­˜ LDA æ¨¡å‹
    LDA.save(lda_model)
    print "æ¨¡å‹å·²ä¿å­˜åˆ° %s ä¸­" % lda_model


def clean_topic(line):
    # ç”¨æ¥æ¸…ç† topic çš„æ˜¾ç¤º
    # example 0.075*"æŠ•" + 0.060*"å¤±è´¥" + 0.059*"æ™ºå®š" + 0.043*"é‚®æ”¿" + 0.036*"æŒä»“" + 0.030*"å¼€é€š" + 0.029*"å®š" + 0.025*"ä¸ºä»€ä¹ˆ" + 0.023*"æ”¯ä»˜" + 0.022*"å¡"
    newline = []
    arr = line.split('+')
    for item in arr:
        words = item.split("\"")
        newline.append(words[1])
    return " ".join(newline)

def show_lda_topic_document(rankid, topicid):
    dictionary = corpora.Dictionary.load(topic_dict_path % topicid)
    print "è½½å…¥ topic %d è¯å…¸å®Œæˆ" % topicid

    # è½½å…¥è¯­æ–™
    texts = get_topic_texts(topicid)

    corpus = [dictionary.doc2bow(text) for text in texts]
    print "Corpus æ•°é‡ %d" % len(corpus)

    # è½½å…¥æ¨¡å‹
    LDA = models.LdaModel.load("%s%d" % (lda_model_topic, topicid))

    # åˆ›å»ºä¸åŒçš„æ•°ç»„ï¼Œæ ¹æ® topic æ¥æ’åº
    topic_result = [{} for i in range(num_sub_topic)]
    for i in range(len(texts)):
        if (i % 1000 == 0):
            print "æ­£åœ¨å¤„ç†ç¬¬ %d è¡Œ" % i 
        # è·å–æ¯ä¸ªæ–‡æœ¬çš„ topics
        topics = LDA.get_document_topics(corpus[i])
        # è¿™é‡Œé€‰æ‹© Top1
        if (len(topics) < 1):
            continue
        # print len(topics), topics[0]
        topic_result[topics[0][0]]["".join(texts[i])] = topics[0][1]

    # å¯¹æ¯ä¸ª topic è¿›è¡Œæ‰“åˆ†
    topic_rank = []
    for i in range(num_sub_topic):
        if (len(topic_result[i]) < 1):
            continue

        temp_doc_vector = []
        # doc score èƒŒæ™¯å™ªå£°å‘é‡
        doc_bg_vector = np.array([1.0 / len(topic_result[i])] * len(topic_result[i]))
        # å¾—å‡º doc score å‘é‡
        for record in topic_result[i].items():
            temp_doc_vector.append(record[1]) # åŠ å…¥å„ä¸ªæ¦‚ç‡
        # å¾—å‡ºæœ€ç»ˆæ¦‚ç‡å‘é‡
        doc_vector = np.array(temp_doc_vector) / sum(temp_doc_vector)

        temp_word_vector = []
        # word score èƒŒæ™¯å™ªå£°å‘é‡
        word_bg_vector = np.array([1.0 / num_topn] * num_topn)
        # å¾—å‡º word score å‘é‡
        for item in LDA.get_topic_terms(i, topn=num_topn):
            temp_word_vector.append(item[1])
        word_vector = np.array(temp_word_vector)
        
        # è®¡ç®—åˆ†æ•°å¹¶æ”¾åˆ°æ•°ç»„ä¸­
        doc_score = asymmetricKL(doc_vector, doc_bg_vector)
        word_score = asymmetricKL(word_vector, word_bg_vector)
        final_score = final_a * doc_score + final_b + word_score
        # print i, final_score, doc_score, word_score
        topic_rank.append((i, final_score))

    # æ ¹æ®åˆ†å€¼è¿›è¡Œæ’åºè¾“å‡º
    # å¯¹æ¯ä¸ªå­—å…¸è¿›è¡Œæ’åº
    ranks = sorted(topic_rank, key=lambda d:-d[1])

    for i in range(len(ranks)):
        line =  LDA.print_topic(ranks[i][0], topn=num_topn)
        line1 = "[Rank %d Topic %d SubRank %d SubTopic %d] %s" % ( rankid, topicid, i, ranks[i][0], clean_topic(line))
        line2 = "åˆ†å€¼ %f å…± %d æ¡ query" % (ranks[i][1], len(topic_result[ranks[i][0]]))
        print line1
        print line2
    
        # æ ¹æ®æ¦‚ç‡æ’åº
        result = sorted(topic_result[ranks[i][0]].items(), key=lambda d:-d[1])

        # å†™å…¥åˆ°å¯¹åº”æ–‡æœ¬ä¸­
        # è¿™é‡Œåº”è¯¥æŒ‰ç…§é¡ºåºå†™å…¥
        filepath = "%srank%d-%d.txt" % (lda_result_dir, rankid, i)
        with codecs.open(filepath, "w+", "utf-8") as f:
            f.write("%s\n%s\n" % (line1, line2))
            f.write("----------------------\n")
            for re in result:
                f.write("%f %s\n" % (re[1], re[0]))
        print "å†™å…¥ %s å®Œæˆ" % filepath
        print "----------------------"


def show_lda_document():
    # è½½å…¥è¯å…¸
    dictionary = corpora.Dictionary.load(dict_path)
    print "è½½å…¥è¯å…¸å®Œæˆ"

    # è½½å…¥è¯­æ–™
    texts = get_texts()

    corpus = [dictionary.doc2bow(text) for text in texts]
    print "Corpus æ•°é‡ %d" % len(corpus)

    LDA = models.LdaModel.load(lda_model)
    print "è½½å…¥ LDA æ¨¡å‹å®Œæˆ"
    
    # åˆ›å»ºä¸åŒçš„æ•°ç»„ï¼Œæ ¹æ® topic æ¥æ’åº
    topic_result = [{} for i in range(num_topic)]
    for i in range(len(texts)):
        if (i % show_count == 0):
            print "æ­£åœ¨å¤„ç†ç¬¬ %d è¡Œ" % i 
        # è·å–æ¯ä¸ªæ–‡æœ¬çš„ topics
        topics = LDA.get_document_topics(corpus[i])
        # è¿™é‡Œé€‰æ‹© Top1
        if (len(topics) < 1):
            continue
        # print len(topics), topics[0]
        topic_result[topics[0][0]]["".join(texts[i])] = topics[0][1]

        # è¿™é‡Œæ˜¯å…¨é‡æ•°æ®
        # for j in range(len(topics)):
        #     # åˆ†é…åˆ°ä¸åŒçš„ topics ä¸­
        #     topic_result[topics[j][0]][texts[i]] = topics[j][1]
    

    # å¯¹æ¯ä¸ª topic è¿›è¡Œæ‰“åˆ†
    topic_rank = []
    for i in range(num_topic):
        if (len(topic_result[i]) < 1):
            continue
        temp_doc_vector = []
        # doc score èƒŒæ™¯å™ªå£°å‘é‡
        doc_bg_vector = np.array([1.0 / len(topic_result[i])] * len(topic_result[i]))
        # å¾—å‡º doc score å‘é‡
        for record in topic_result[i].items():
            temp_doc_vector.append(record[1]) # åŠ å…¥å„ä¸ªæ¦‚ç‡
        # å¾—å‡ºæœ€ç»ˆæ¦‚ç‡å‘é‡
        doc_vector = np.array(temp_doc_vector) / sum(temp_doc_vector)

        temp_word_vector = []
        # word score èƒŒæ™¯å™ªå£°å‘é‡
        word_bg_vector = np.array([1.0 / num_topn] * num_topn)
        # å¾—å‡º word score å‘é‡
        for item in LDA.get_topic_terms(i, topn=num_topn):
            temp_word_vector.append(item[1])
        word_vector = np.array(temp_word_vector)
        
        # è®¡ç®—åˆ†æ•°å¹¶æ”¾åˆ°æ•°ç»„ä¸­
        doc_score = asymmetricKL(doc_vector, doc_bg_vector)
        word_score = asymmetricKL(word_vector, word_bg_vector)
        final_score = final_a * doc_score + final_b + word_score
        # print i, final_score, doc_score, word_score
        topic_rank.append((i, final_score))

    # æ ¹æ®åˆ†å€¼è¿›è¡Œæ’åºè¾“å‡º
    # å¯¹æ¯ä¸ªå­—å…¸è¿›è¡Œæ’åº
    ranks = sorted(topic_rank, key=lambda d:-d[1])

    for i in range(len(ranks)):
        line =  LDA.print_topic(ranks[i][0], topn=num_topn)
        line1 = "[Rank %d Topic %d] %s" % (i, ranks[i][0], clean_topic(line))
        line2 = "åˆ†å€¼ %f å…± %d æ¡ query" % (ranks[i][1], len(topic_result[ranks[i][0]]))
        print line1
        print line2
    
        # æ ¹æ®æ¦‚ç‡æ’åº
        result = sorted(topic_result[ranks[i][0]].items(), key=lambda d:-d[1])

        # å†™å…¥åˆ°å¯¹åº”æ–‡æœ¬ä¸­
        # è¿™é‡Œåº”è¯¥æŒ‰ç…§å¾—åˆ†é¡ºåºå†™å…¥ï¼ŒåŠ ä¸€ä¸ª - ç”¨æ¥æ’åº
        filepath = "%srank%d-.txt" % (lda_result_dir, i)
        with codecs.open(filepath, "w+", "utf-8") as f:
            f.write("%s\n%s\n" % (line1, line2))
            f.write("----------------------\n")
            for re in result:
                f.write("%f %s\n" % (re[1], re[0]))
        print "å†™å…¥ %s å®Œæˆ" % filepath
        print "----------------------"

        # è¯»å…¥è¿™ä¸ªåˆ†ç±»ä¸‹çš„å­åˆ†ç±»
        # show_lda_topic_document(i, ranks[i][0])

def show_lsi_document():
     # è½½å…¥è¯å…¸
    dictionary = corpora.Dictionary.load(dict_path)
    print "è½½å…¥è¯å…¸å®Œæˆ"

    # è½½å…¥è¯­æ–™
    texts = get_texts()

    corpus = [dictionary.doc2bow(text) for text in texts]
    print "Corpus æ•°é‡ %d" % len(corpus)

    LSI = models.LsiModel.load(lsi_model)
    print "è½½å…¥ LSI æ¨¡å‹å®Œæˆ"

    filepath = "%slsi-topics.txt" % lsi_result_dir
    with codecs.open(filepath, "w+", "utf-8") as f:
        for i in range(num_topic):
            topic = LSI.show_topic(i, topn=40)
            f.write("---------------------\n")
            f.write("Topic %d\n" % i)
            for term in topic:
                f.write("%s %f\n" % (term[0], term[1]))
    
    # å¾—åˆ°æ–‡æ¡£çš„å‘é‡ï¼Œå…± num_topic ç»´ï¼Œç”Ÿæˆæµ‹è¯•æ•°æ®
    print "ç”Ÿæˆæ–‡æ¡£å‘é‡"
    # è¿™é‡Œçš„æ ¼å¼éœ€è¦å¤„ç† ç±»å‹ä¸åŒ
    
    X = np.zeros((len(corpus), num_topic))

    for i in range(len(corpus)):
        vline = LSI[corpus[i]]
        for j in range(len(vline)):
            X[i][j] = vline[j][1]

    print "å½’ä¸€åŒ–æ•°æ®é›†ï¼ˆç‰¹å¾é€‰æ‹©ï¼‰"
    # normalized dataset for easier parameter selection
    X = StandardScaler().fit_transform(X)

    kmeans = cluster.MiniBatchKMeans(n_clusters=num_topic)
    #affinity_propagation = cluster.AffinityPropagation(damping=.5, preference=None)

    t0 = datetime.datetime.now()
    print "å¼€å§‹ Kmean èšç±»ï¼Œä¸­å¿ƒä¸ªæ•° %d" % num_topic
    kmeans.fit(X)
    #print "å¼€å§‹ AP èšç±»ï¼Œä¸­å¿ƒä¸ªæ•° %d" % num_topic
    #affinity_propagation.fit(X)
    t1 = datetime.datetime.now()
    print "èšç±»è€—æ—¶", t1-t0
    
    # è¾“å‡ºç»“æœ
    print "æŒ‰ç…§ç±»åˆ«å†™å…¥åˆ°ç»“æœä¸­"
    y_pred = kmeans.labels_.astype(np.int)
    #y_pred = affinity_propagation.labels_.astype(np.int)

    # å„ä¸ªç±»åˆ«çš„ç»“æœ
    # ä¹Ÿå†™å…¥åˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ–‡ä»¶ä¸­
    resultpath = "%sresult_all.txt" % lsi_result_dir
    with codecs.open(resultpath, "w+", "utf-8") as r:
        topic_result = [[] for i in range(num_topic)]
        for i in range (len(texts)):
            topic_result[y_pred[i]].append(texts[i])
        for i in range (len(topic_result)):
            filepath = "%stopic%d.txt" % (lsi_result_dir, i)
            print "å†™å…¥ç±»åˆ« %d è‡³ %s" % (i, filepath)
            with codecs.open(filepath, "w+", "utf-8") as f:
                r.write("ç±»åˆ« %d çš„è®°å½•ä¸ªæ•° %d\n" % (i, len(topic_result[i])))
                f.write("ç±»åˆ« %d çš„è®°å½•ä¸ªæ•° %d\n" % (i, len(topic_result[i])))
                for line in topic_result[i]:
                    f.write("".join(line)+"\n")
                    r.write("".join(line)+"\n")
                # æ€»æ–‡ä»¶åŠ ä¸ªåˆ†éš”ç¬¦
                r.write("===============================================\n") 
        print "LSI ç»“æœå¤„ç†å®Œæˆ"

# TODO æŒ‰ç…§èšç±»ä¸­å¿ƒè·ç¦»å¯¹é—®å¥æ’åº
def cluster_query(method):
    # ç”¨æ¥èšç±»æ¨è€å¸ˆé‚£è¾¹ç»™å‡ºçš„æ•°æ®
    load_raw_query()
    load_hidden_vector()
    # æ£€æµ‹æ•°é‡åŒ¹é…
    if (len(documents) != len(hidden_vectors)):
        print "æ—¥å¿—æ•°é‡ä¸å‘é‡æ•°é‡ä¸ç¬¦ï¼Œè¯·æ£€æŸ¥åé‡è¯•"
        sys.exit()
    
    # æ¥ä¸‹æ¥æ˜¯æ­£å¸¸å¤„ç†æµç¨‹
    print "ç”Ÿæˆéšè—å‘é‡æ•°ç»„"
    t0 = datetime.datetime.now()
    X = np.array([[ele for ele in vector[:-1].split("\t")] 
            for vector in hidden_vectors])
    t1 = datetime.datetime.now()
    print "è€—æ—¶", t1-t0

    # print "å½’ä¸€åŒ–æ•°æ®é›†ï¼ˆç‰¹å¾é€‰æ‹©ï¼‰"
    # # normalized dataset for easier parameter selection
    # t0 = datetime.datetime.now()
    # X = StandardScaler().fit_transform(X)
    # t1 = datetime.datetime.now()
    # print "è€—æ—¶", t1-t0

    if (method=="kmeans"):
        print "å¼€å§‹ %s èšç±»ï¼Œä¸­å¿ƒä¸ªæ•° %d" % (method, num_topic)
        algorithm = cluster.MiniBatchKMeans(n_clusters=num_topic)
    elif (method == "ap"):
        print "å¼€å§‹ %s èšç±»ï¼Œä¸­å¿ƒä¸ªæ•°å¾…å®š" % method
        algorithm = cluster.AffinityPropagation(damping=.5, preference=None)
    
    t0 = datetime.datetime.now()
    algorithm.fit(X)
    t1 = datetime.datetime.now()
    print "è€—æ—¶", t1-t0
    
    # è¾“å‡ºç»“æœ
    print "æŒ‰ç…§ç±»åˆ«å†™å…¥åˆ°ç»“æœä¸­"
    y_pred = algorithm.labels_.astype(np.int)

    # èšç±»ä¸­å¿ƒ
    centers = algorithm.cluster_centers_
    print centers


    # æ‰¾åˆ°ç±»åˆ«ä¸­çš„æœ€å¤§å€¼
    maxY = max(y_pred)
    print "ç±»åˆ«ä¸ªæ•°", maxY+1
    
    # å„ä¸ªç±»åˆ«çš„ç»“æœ
    # ä¹Ÿå†™å…¥åˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ–‡ä»¶ä¸­
    resultpath = "%sresult_all.txt" % kmeans_result_dir
    with codecs.open(resultpath, "w+", "utf-8") as r:
        topic_result = [{} for i in range(maxY+1)]
        print "è®¡ç®—æ–‡æ¡£åˆ°ç±»ä¸­å¿ƒçš„è·ç¦»..."
        for i in range (len(documents)):
            dist = np.linalg.norm(centers[y_pred[i]].astype('float64') - X[i].astype('float64')) 
            topic_result[y_pred[i]][documents[i]] = dist
        print "å†™å…¥ç»“æœ..."
        for i in range (len(topic_result)):
            filepath = "%stopic%d.txt" % (kmeans_result_dir, i)
            # print "å†™å…¥ç±»åˆ« %d è‡³ %s" % (i, filepath)
            with codecs.open(filepath, "w+", "utf-8") as f:
                r.write("ç±»åˆ« %d çš„è®°å½•ä¸ªæ•° %d\n" % (i, len(topic_result[i])))
                f.write("ç±»åˆ« %d çš„è®°å½•ä¸ªæ•° %d\n" % (i, len(topic_result[i])))
                r.write("èšç±»ä¸­å¿ƒ %s\n" % np.array2string(centers[i]))
                f.write("èšç±»ä¸­å¿ƒ %s\n" % np.array2string(centers[i]))
                # æ’åºå¹¶è¾“å‡º
                sorted_list = sorted(topic_result[i].items(), key=lambda item:item[1])
                for line in sorted_list:
                    f.write("%f %s\n" % (line[1], line[0]))
                    r.write("%f %s\n" % (line[1], line[0]))
                # æ€»æ–‡ä»¶åŠ ä¸ªåˆ†éš”ç¬¦
                r.write("===============================================\n") 

    print "èšç±»ç»“æœå¤„ç†å®Œæˆ"

# è¿™é‡Œæœ€å¥½éœ€è¦è¿›è¡Œä¸€æ­¥ dos2unix filename è½¬æ¢æ¢è¡Œç¬¦
def word_freq():
    total = len(documents)
    print "ç»Ÿè®¡ %d æ¡æ•°æ®çš„è¯é¢‘" % total
    t0 = datetime.datetime.now()
    word_dict = {} 
    i = 0
    word_count = 0
    for doc in documents:
        if i % 5000 == 0:
            print "æ­£åœ¨å¤„ç†ç¬¬ %d/%d è¡Œ" % (i, total)
        i += 1
        words = doc.split(' ')
        for word in words:
            word_count += 1 # ç»Ÿè®¡æ€»è¯æ•°
            if word not in word_dict: 
                word_dict[word] = 1  
            else:  
                word_dict[word] += 1
        
    with codecs.open(word_freq_file, "w", "utf-8") as f:
        f.write("æ€»è¯è¯­ä¸ªæ•° %d\n------------------\n" % word_count)
        sorted_list = sorted(word_dict.items(), key=lambda item:-item[1])
        for item in sorted_list:
            # \r å°±æ˜¯ ^M å°±æ˜¯ windows çš„æ¢è¡Œç¬¦
            if item == ' ' or item[0] == '\n' or item[0] == '\t' or item[0] == '\r':
                continue
            f.write("%s\t%d\n" % (item[0], item[1]))
    t1 = datetime.datetime.now()
    print "è¯é¢‘ç»Ÿè®¡å®Œæˆï¼Œå…±è€—æ—¶", t1-t0

def parse_query_answer():
    print "å¤„ç†é—®ç­”å¯¹ï¼Œæ•°æ®æº %s" % query_answer_rawdata
    count = 0
    step = 0
    min_length = 20 # æœ€å°‘è¦å››ä¸ªæ±‰å­—ï¼Œé•¿åº¦ä¸º 20
    min_word_count = 6 # æœ€å°‘è¯è¯­æ•°ç›®ï¼Œå¸¦ä¸Šæ ‡ç‚¹è‡³å°‘è¦ 6 ä¸ª
    question = ""
    answer = ""
    result = []
    question_list = []
    answer_list = []
    t0 = datetime.datetime.now()
    with open(query_answer_rawdata) as fr:
        print "å¼€å§‹å¤„ç†"
        for line in fr:
            
            #arr = line.split('\t')
            arr = line.split('##')
            #print arr
            # å¦‚æœåˆ†éš”åé•¿åº¦ä¸ä¸º 4ï¼Œé‚£ä¹ˆè¯´æ˜æ˜¯ä¸Šä¸€å¥ä¸­æœ‰æ¢è¡Œ
            # åˆ™ç›´æ¥æ ¹æ®å½“å‰æ‰€å±çŠ¶æ€æ·»åŠ åˆ°å¯¹åº”çš„å¥å­ä¸­

            # step 0 è¡¨ç¤ºåˆå§‹çŠ¶æ€ï¼Œå³ç”¨æˆ·æ²¡æœ‰å‘é—®ï¼Œç³»ç»Ÿæ²¡æœ‰ä½œç­”
            # è¿™æ—¶åªæœ‰é‡åˆ°ç”¨æˆ·é—®å¥ï¼Œæ‰è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
            if step == 0:
                #if arr[2] != 'å®¢æˆ·':
                if arr[0] == 'ç™½ç‰åŒ-åˆä½œæ–¹':
                    continue
                # è¿›å…¥è¿™é‡Œè¡¨ç¤ºå¼€å§‹ç”¨æˆ·é—®å¥
                question = arr[1][:-1] # -1 å»æ‰æ¢è¡Œç¬¦
                step = 1 # è¡¨ç¤ºè¿›å…¥ step 1ï¼Œç”¨æˆ·é—®å¥é˜¶æ®µ
            # step 1 è¡¨ç¤ºç”¨æˆ·é—®å¥çŠ¶æ€ï¼Œå¦‚æœä¸‹ä¸€å¥è¿˜æ˜¯ç”¨æˆ·é—®å¥ï¼Œé‚£ä¹ˆç›´æ¥å åŠ åˆ° question
            # å¦‚æœä¸‹ä¸€å¥æ˜¯å®¢æœå›ç­”ï¼ˆéæœºå™¨äººå›ç­”ï¼‰ï¼Œé‚£ä¹ˆè¿›å…¥ step 2
            elif step == 1:
                #print step
                if len(arr) < 2:
                    #question += line[:-1].strip()
                    continue
                print count
                print arr 
                if arr[0] != 'ç™½ç‰åŒ-åˆä½œæ–¹': # å¦‚æœè¿™ä¸€å¥è¿˜æ˜¯ç”¨æˆ·é—®å¥
                    question += " " + arr[1][:-1]
                elif arr[0] == 'ç™½ç‰åŒ-åˆä½œæ–¹': 
                    # å¦‚æœæ˜¯å®¢æœå›ç­”ï¼ˆå¹¶æ»¡è¶³è¿‡æ»¤æ¡ä»¶ï¼‰ï¼Œåˆ™åŠ å…¥ answerï¼Œå¹¶è¿›å…¥ step 2
                    answer = arr[1][:-1]
                    step = 2
            # step 2 è¡¨ç¤ºå®¢æœå›ç­”çŠ¶æ€ï¼Œå¦‚æœä¸‹ä¸€å¥è¿˜æ˜¯å®¢æœå›ç­”ï¼Œé‚£ä¹ˆç›´æ¥å åŠ 
            # å¦‚æœä¸‹ä¸€å¥æ˜¯ç”¨æˆ·é—®å¥ï¼Œåˆ™è®¤ä¸ºä¸€ä¸ªé—®ç­”å¯¹å·²ç»å®Œæˆï¼Œæ·»åŠ åˆ°è¦å†™å…¥çš„æ•°æ®ä¸­ï¼Œå¹¶æ¢å¤åˆå§‹çŠ¶æ€
            elif step == 2:
                #print step
                #if len(arr) < 4:
                #    answer += line[:-1].strip()
                #    continue
                if arr[0] == 'ç™½ç‰åŒ-åˆä½œæ–¹':
                    answer += " " + arr[1][:-1]
                if arr[0] != 'ç™½ç‰åŒ-åˆä½œæ–¹':
                    # å…ˆç”¨ç›®å‰çš„é—®ç­”å¯¹æ‹¼æ¥æˆé—®å¥ï¼ˆéœ€è¦åˆ†è¯ï¼‰
                    question = question.replace("\n", ";")
                    question = question.replace("\r", "")
                    answer = answer.replace("\n", ";")
                    answer = answer.replace("\r", "")

                    if len(question) < min_length:
                        # å¦‚æœç”¨æˆ·é—®å¥è¿‡çŸ­ï¼Œé‚£ä¹ˆå¿½ç•¥
                        question = arr[1][:-1] # -1 å»æ‰æ¢è¡Œç¬¦
                        step = 1 # è¡¨ç¤ºè¿›å…¥ step 1ï¼Œç”¨æˆ·é—®å¥é˜¶æ®µ
                        continue

                    qarr = jieba.cut(question)  # é»˜è®¤æ˜¯ç²¾ç¡®æ¨¡å¼
                    q_seg = " ".join(qarr)
                    if len(q_seg.split(" ")) < min_word_count:
                        # å¦‚æœè¯è¿‡å°‘ï¼Œé‚£ä¹ˆå¿½ç•¥
                        question = arr[1][:-1] # -1 å»æ‰æ¢è¡Œç¬¦
                        step = 1 # è¡¨ç¤ºè¿›å…¥ step 1ï¼Œç”¨æˆ·é—®å¥é˜¶æ®µ
                        continue
                    
                    aarr = jieba.cut(answer)
                    a_seg = " ".join(aarr)

                    content = q_seg + "##" + a_seg

                    # åŠ å…¥åˆ°æ•°ç»„ä¸­ï¼Œç­‰å¾…æœ€åå†™å…¥
                    result.append(content)
                    #print result
                    question_list.append(q_seg)
                    answer_list.append(a_seg)
                    
                    count = count + 1
                    if count % 1000 == 0:
                        print "å·²å¤„ç† %d ä¸ªé—®ç­”å¯¹" % count
                    # æ¢å¤åˆ° step 1
                    question = arr[1][:-1] # -1 å»æ‰æ¢è¡Œç¬¦
                    step = 1 # è¡¨ç¤ºè¿›å…¥ step 1ï¼Œç”¨æˆ·é—®å¥é˜¶æ®µ
    
    print "æ­£åœ¨å†™å…¥ç»“æœåˆ° %s" % query_answer_result
    with codecs.open(query_answer_result, "w", "utf-8") as f:
        for line in result:
            try:
                f.write("%s\n" % line)
            except :
                pass
    print "æ­£åœ¨å†™å…¥é—®é¢˜åˆ° %s" % query_answer_qfile
    with codecs.open(query_answer_qfile, "w", "utf-8") as fq:
        for line in question_list:
            try:
                fq.write("%s\n" % line)
            except :
                pass
    print "æ­£åœ¨å†™å…¥ç­”æ¡ˆåˆ° %s" % query_answer_afile
    with codecs.open(query_answer_afile, "w", "utf-8") as fa:
        for line in answer_list:
            try:
                fa.write("%s\n" % line)
            except :
                pass  
    t1 = datetime.datetime.now()
    print "å…± %d æ¡é—®ç­”å¯¹" % count
    print "é—®ç­”å¯¹å®Œæˆï¼Œå…±è€—æ—¶", t1-t0

def parse_knowledge_data():
    print "å¼€å§‹è§£æçŸ¥è¯†åº“å¹¶ç”Ÿæˆé—®ç­”å¯¹"
    t0 = datetime.datetime.now()
    qcount = 0
    acount = 0

    knowledge_xlsx = xlrd.open_workbook(knowledge_data)
    sheet = knowledge_xlsx.sheet_by_index(0)
    # sheetçš„åç§°ï¼Œè¡Œæ•°ï¼Œåˆ—æ•°
    print "è¡¨åç§°", sheet.name
    print "å…± %d è¡Œï¼Œ%d åˆ—" % (sheet.nrows, sheet.ncols)
    result = []
    # è·å–æ•´è¡Œå’Œæ•´åˆ—çš„å€¼ï¼ˆæ•°ç»„ï¼‰
    for i in range(sheet.nrows):
        rows = sheet.row_values(i) # è·å–ç¬¬å››è¡Œå†…å®¹
        result.append(rows)
    #print rows
    #questions = [] 
    # è·å–æ ‡å‡†é—®å¥
    #for q in sheet.col_values(4)[1:]:
    #    qcount += 1
    #    q = q.replace("\r", "")
    #    q = q.replace("\n", "")
    #    q = q.strip()
    #    qarr = jieba.cut(q)
    #    questions.append(" ".join(qarr))

    # è·å–æ ‡å‡†å›ç­”
    #answers = []
    #for a in sheet.col_values(5)[1:]:
    #    acount += 1
    #    a = a.replace("\r", "")
    #    a = a.replace("\n", "")
    #    a = a.replace("<br>", "")
    #    a = a.strip()
    #    aarr = jieba.cut(a)
    #    answers.append(" ".join(aarr))

    print "æ­£åœ¨å†™å…¥ç»“æœåˆ° %s" % knowledge_result
    with codecs.open(knowledge_result, "w", "utf-8") as f:
    #    for i in range(qcount):
        for i in range(sheet.nrows):
            try:
                #f.write("%s##%s\n" % (questions[i], answers[i]))
                f.write("%s##%s\n" % (result[i][0], result[i][2]))
            except :
                pass
    
    #t1 = datetime.datetime.now()
    #print "å…± %d ä¸ªæ ‡å‡†é—®å¥ï¼Œ%d ä¸ªæ ‡å‡†å›ç­”" % (qcount, acount)
    #print "é—®ç­”å¯¹å®Œæˆï¼Œå…±è€—æ—¶", t1-t0
    

def split_word():
    load_raw_query()
    print "å¼€å§‹å†™å…¥åˆ†è¯åçš„ç»“æœ"
    with codecs.open(split_query, "w+", "utf-8") as r:
        for i in range (len(documents)):
            arr = jieba.cut(documents[i])
            newline = " ".join(arr)
            newline = newline[:-1] + "##\n"
            r.write(newline)
    print "ä¸º seq2bow æ•°æ®å‡†å¤‡å®Œæ¯•"

emoji_pattern = re.compile(
    u"(\ud83d[\ude00-\ude4f])|"  # emoticons
    u"(\ud83c[\udf00-\uffff])|"  # symbols & pictographs (1 of 2)
    u"(\ud83d[\u0000-\uddff])|"  # symbols & pictographs (2 of 2)
    u"(\ud83d[\ude80-\udeff])|"  # transport & map symbols
    u"(\ud83c[\udde0-\uddff])"  # flags (iOS)
    "+", flags=re.UNICODE)

def remove_emoji(text):
    return emoji_pattern.sub(r'', text)

def help():
    print "ç”¨æˆ·é—®å¥èšç±»æµ‹è¯•"
    print "ç”¨æ³• python topic_cluster.py dict|tlda|tlsi|slda|slsi|kmeans|ap|help"
    print "dict - ç”Ÿæˆè¯å…¸å¹¶ä¿å­˜åˆ° %s ä¸­ï¼ŒåŸå§‹åˆ†è¯æ•°æ®åœ¨ %s ä¸­" % (dict_path, tokenized_query)
    print "tlda - ç”¨ LDA æ¨¡å‹ç”Ÿæˆ %d ä¸ª topicï¼Œæ¨¡å‹ä¿å­˜åœ¨ %s ä¸­" % (num_topic, lda_model)
    print "tlsi - ç”¨ LSI æ¨¡å‹ç”Ÿæˆ %d ä¸ª topicï¼Œæ¨¡å‹ä¿å­˜åœ¨ %s ä¸­" % (num_topic, lsi_model)
    print "slda - ç»Ÿè®¡ LDA èšç±»å‡ºæ¥çš„ topic å¹¶ä¿å­˜åœ¨ %s ä¸­" % lda_result_dir
    print "slsi - ç»Ÿè®¡ LSI èšç±»å‡ºæ¥çš„ topic å¹¶ä¿å­˜åœ¨ %s ä¸­" % lsi_result_dir
    print "kmeans -  åˆ©ç”¨ Kmeans èšç±»ç»“æœå¹¶ä¿å­˜åœ¨ %s ä¸­" % kmeans_result_dir
    print "ap - åˆ©ç”¨ AP èšç±»ç»“æœå¹¶ä¿å­˜åœ¨ %s ä¸­" % ap_result_dir
    print "wordfreq - ç»Ÿè®¡è¯é¢‘å¹¶ä¿å­˜åœ¨ %s ä¸­" % word_freq_file
    print "qa - æŠŠ %s ä¸­çš„æ•°æ®å¤„ç†æˆé—®ç­”å¯¹å¹¶ä¿å­˜åœ¨ %s ä¸­" % (query_answer_rawdata, query_answer_result)
    print "parse - å¤„ç† %s çŸ¥è¯†åº“çš„é—®ç­”å¯¹å¹¶ä¿å­˜åœ¨ %s ä¸­" % (knowledge_data, knowledge_result)
    print "split - åˆ†è¯ %s çš„å†…å®¹å¹¶æ·»åŠ  ## ç¬¦å·ï¼ˆä¸º seq2bow å·¥å…·åšæ•°æ®å‡†å¤‡ï¼‰ä¿å­˜åœ¨ %s ä¸­" % (raw_query, split_query)

if __name__=="__main__":
    if len(argv) != 2:
        help()
        sys.exit()
    if (argv[1] == "dict"):
        tokenize_query()
        generate_dict() 
    elif (argv[1] == "tlda"):
        load_tokenized_query()
        generate_lda_topic() 
    elif (argv[1] == "tlsi"):
        load_tokenized_query()
        generate_lsi_topic() 
    elif (argv[1] == "slda"):
        load_tokenized_query()
        show_lda_document()
    elif (argv[1] == "slsi"):
        load_tokenized_query()
        show_lsi_document()
    elif (argv[1] == "kmeans"):
        cluster_query(argv[1])
    elif (argv[1] == "ap"):
        cluster_query(argv[1])
    elif (argv[1] == "help"):
        help()
    elif (argv[1] == "wordfreq"):
        load_tokenized_query()
        word_freq()
    elif (argv[1] == "qa"):
        parse_query_answer()
    elif (argv[1] == 'parse'):
        parse_knowledge_data()
    elif (argv[1] == 'split'):
        split_word()
    else:
        print "æœªçŸ¥å‘½ä»¤"
        help()
